<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" type="text/css" href="portfolio.css">
    <link href="https://fonts.googleapis.com/css?family=Overpass:200,400" rel="stylesheet">
    <title>.: jlm :. Knowledge, Now!</title>
</head>

<body class="wikify">

<div class="center-content">
    <h1>Minerva - knowledge, now</h2>
    <h2>I've always found it a bit shocking that something like Wikipedia — an astounding compendium of human knowledge — is not more integrated into our daily lives. It wasn't until recently that Google search even incorporated Wikipedia articles into their search results. There's no way, at the moment, to augment your life — and the things you see and read — with the additional layer of information offered by something like Wikipedia</h2>
    <h2>In order to fix this problem, I've begun building Minerva. The goal is to surface relevant knowledge automatically and naturally, without having to explicitly search for something.</h2>
    <h2>By leveraging state-of-the-art neural NLP techniques, Minerva is able to use a Naive Bayes Classifier to solve a problem the local Disambiguation to Wikipedia problem in a way that is simple, elegant, and fast enough to run in a web browser.</h2>
    <h2>>>></h2>
    <h3>Phase 1: Disambiguation to Wikipedia (complete!)</h3>
    <div class='pagesection'>
        <table>
            <tr>
                <p>
                    In order to do this, I use a Naive Bayes Classifier that relies on rich pre-trained embeddings and precomputed probabilities. Passing over the entirety of Wikipedia, I first computed probabilities for occurrences of links to articles, as well as specific surface forms linking to specific articles. I also resolved the redirect graph. Then, using learned word embeddings, I computed an average local context for every instance of a link to an article. At runtime, for a given surface form that the classifier is trying to link to some article, we uses the precomputed probabilities as well as a distribution computed from comparing context distances between candidate articles’ context embeddings.
                </p>
                <p>
                    Because all of this work is done prior to runtime, and because it gives an explicit formula for each candidate article score, inference is nearly instantaneous. I'll be experimenting with classifier tuning to see how this approach compares with standard benchmarks for the task. You can see the code that builds the classifier <a href='https://github.com/jmilbauer/minerva'>here</a>.
                </p>
            </tr>
            <tr>
                <td><img src='images/minerva_pipeline.png' style='width: 60vw;'></td>
            </tr>
        </table>
    </div>
    <h3>Phase 1.2: Deep Learning<h4>
        <div class='pagesection'>
            <p>
                Next up, I'm going to move away from the naive bayes classifier to actually tune weights for the various parameters — title probability, surface link -> title probability, context embeddings — for a neural network classifier.
            </p>
        </div>
    <h3>Phase 1.3: Browser compatibility</h3>
    <h3>Phase 2: Keyword Recognition</h3>
    <h3>Phase 3: Browser Extension</h3>
    <h3>Phase n: Further knowledge layers.</h3>
</div>

</body>

</html>
